{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0fca1219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'network' from 'd:\\\\User\\\\ProjectGithub\\\\hiepnguyenn-99\\\\Chess\\\\RL\\\\network.py'>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import importlib\n",
    "import ChessEngine\n",
    "import Minimax.SmartMoveFinder as SmartMoveFinder\n",
    "import Minimax.Evaluate as Evaluate\n",
    "import ChessEnv\n",
    "import replaybuffer\n",
    "import network\n",
    "import copy \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "importlib.reload(ChessEngine)\n",
    "importlib.reload(ChessEnv)\n",
    "importlib.reload(replaybuffer)\n",
    "importlib.reload(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f4805484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Đã load model\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv.Env()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "q_net = network.DQN(env.action_size).to(device)\n",
    "\n",
    "# load mô hình lưu\n",
    "model_path = 'DQN.pth'\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    q_net.load_state_dict(checkpoint)\n",
    "    q_net.train()\n",
    "    print(\"Đã load model\")\n",
    "else:\n",
    "    print(\"Không tìm thấy model\")\n",
    "\n",
    "target_net = copy.deepcopy(q_net).to(device)\n",
    "target_net.eval()\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "criterion  = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b4547fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white True move 7152\n",
      "len move black 25\n",
      "Các chỉ số có giá trị True trong before_legal_mask: [0, 1, 2, 254, 255, 256, 257, 446, 447, 519, 527, 583, 591, 647, 655, 711, 719, 775, 783, 839, 847, 903, 911, 967, 975]\n",
      "Index: 0 -> MoveID: 1\n",
      "Index: 1 -> MoveID: 2\n",
      "Index: 2 -> MoveID: 3\n",
      "Index: 254 -> MoveID: 402\n",
      "Index: 255 -> MoveID: 403\n",
      "Index: 256 -> MoveID: 405\n",
      "Index: 257 -> MoveID: 406\n",
      "Index: 446 -> MoveID: 705\n",
      "Index: 447 -> MoveID: 706\n",
      "Index: 519 -> MoveID: 1020\n",
      "Index: 527 -> MoveID: 1030\n",
      "Index: 583 -> MoveID: 1121\n",
      "Index: 591 -> MoveID: 1131\n",
      "Index: 647 -> MoveID: 1222\n",
      "Index: 655 -> MoveID: 1232\n",
      "Index: 711 -> MoveID: 1323\n",
      "Index: 719 -> MoveID: 1333\n",
      "Index: 775 -> MoveID: 1424\n",
      "Index: 783 -> MoveID: 1434\n",
      "Index: 839 -> MoveID: 1525\n",
      "Index: 847 -> MoveID: 1535\n",
      "Index: 903 -> MoveID: 1626\n",
      "Index: 911 -> MoveID: 1636\n",
      "Index: 967 -> MoveID: 1727\n",
      "Index: 975 -> MoveID: 1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type states <class 'torch.Tensor'>, shape torch.Size([1, 13, 8, 8])\n",
      "type actions <class 'torch.Tensor'>, shape torch.Size([1, 1])\n",
      "actions tensor([[447]], device='cuda:0')\n",
      "type reward <class 'torch.Tensor'>, shape torch.Size([1, 1])\n",
      "reward tensor([[0.]], device='cuda:0')\n",
      "type next_states  <class 'torch.Tensor'>, shape torch.Size([1, 13, 8, 8])\n",
      "type before_legal_masks <class 'torch.Tensor'>, shape torch.Size([1, 4032])\n",
      "before_legal_masks tensor([[ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n",
      "type next_legal_masks <class 'torch.Tensor'>, shape torch.Size([1, 4032])\n",
      "next_legal_masks tensor([[ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n",
      "type dones <class 'torch.Tensor'>, shape torch.Size([1, 1])\n",
      "dones tensor([[0.]], device='cuda:0')\n",
      "type q_target <class 'torch.Tensor'>, shape torch.Size([1, 1])\n",
      "q_target tensor([[0.9598]], device='cuda:0')\n",
      "type q_values <class 'torch.Tensor'>, shape torch.Size([1, 4032])\n",
      "q_values tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "type q_value <class 'torch.Tensor'>, shape torch.Size([1, 1])\n",
      "q_values tensor([[1.0430]], device='cuda:0', grad_fn=<GatherBackward0>)\n",
      "loss 0.0069233570247888565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv.Env()\n",
    "capacity = 1\n",
    "rb = replaybuffer.ReplayBuffer(capacity)\n",
    "batch_size = 1\n",
    "epsilon = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.9\n",
    "step = 0\n",
    "target_update_freq = 1\n",
    "env.gs.whiteToMove = True # train trắng đi trước (minimax)\n",
    "\n",
    "for episode in trange(1, desc=\"Episodes\"):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        # nước đi của minimax\n",
    "        white_capture = None # kiểm tra minimax có ăn quân của rl không\n",
    "        if env.gs.whiteToMove:\n",
    "            if Evaluate.check_mid_game(env.gs):\n",
    "                SmartMoveFinder.DEPTH = 4\n",
    "            else:\n",
    "                SmartMoveFinder.DEPTH = 3\n",
    "            MinimaxMove = SmartMoveFinder.findBestMinimaxMove(env.gs, env.gs.getValidMoves())\n",
    "            if MinimaxMove is None:\n",
    "                MinimaxMove = SmartMoveFinder.findRandomMove(env.gs.getValidMoves())\n",
    "            print(f'white {env.gs.whiteToMove} move {MinimaxMove.moveID}')\n",
    "            if MinimaxMove.pieceCaptured != '--':\n",
    "                white_capture = MinimaxMove.pieceCaptured[1]\n",
    "                print(f'white captured {white_capture}')\n",
    "            env.gs.makeMove(MinimaxMove)\n",
    "\n",
    "        state_tensor = env.state_to_tensor()\n",
    "        lenlegalmove = len(env.gs.getValidMoves())\n",
    "        print(f'len move black {len(env.gs.getValidMoves())}')\n",
    "        # khám phá\n",
    "        if lenlegalmove != 0:    \n",
    "            if random.random() < epsilon:\n",
    "                move = random.choice(env.gs.getValidMoves())\n",
    "                action = env.moveid_to_index[move.moveID]\n",
    "                # print(f'white {env.gs.whiteToMove} move {move.moveID}')\n",
    "\n",
    "            # khai thác\n",
    "            else:\n",
    "                q_value = q_net(state_tensor.unsqueeze(0).to(device)) \n",
    "                action = q_value.argmax()\n",
    "                    \n",
    "        next_state_tensor, reward, done, before_legal_mask, after_legal_mask = env.step(action, white_capture, lenlegalmove) # đen đã đi, kiểm tra đen bị ăn quân ko\n",
    "        rb.push(state_tensor, action, reward, next_state_tensor, done, before_legal_mask, after_legal_mask)\n",
    "\n",
    "        if rb.__len__() >= batch_size:\n",
    "            batch = rb.sample(batch_size)\n",
    "            # chuyển về tensor\n",
    "            states, actions, reward, next_states, dones, before_legal_masks, after_legal_mask= zip(*batch)\n",
    "            states = torch.stack([s.to(device) for s in states])\n",
    "            actions = torch.tensor(actions, device=device, dtype=torch.int64).unsqueeze(1) # (B, 1)\n",
    "            reward = torch.tensor(reward, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.stack([ns.to(device) for ns in next_states])\n",
    "            before_legal_masks = torch.stack([b.to(device) for b in before_legal_masks])\n",
    "            after_legal_mask = torch.stack([a.to(device) for a in after_legal_mask])\n",
    "            dones = torch.tensor(dones, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states) # (B, action_size)\n",
    "                next_q_values[~after_legal_mask] = -torch.inf\n",
    "                # print(f'legal_masks[78] {after_legal_mask[0, 78].item()}')\n",
    "                next_q_max = next_q_values.max(1)[0].unsqueeze(1) # (B, 1)\n",
    "\n",
    "                # nếu không còn nước đi hợp lệ của next_states thì đặt bằng 0\n",
    "                legal_exists = after_legal_mask.any(dim=1, keepdim=True)\n",
    "                next_q_max = torch.where(legal_exists, next_q_max, torch.zeros_like(next_q_max))\n",
    "\n",
    "                q_target = reward if done else reward + gamma * next_q_max # (B, 1)\n",
    "\n",
    "            q_values = q_net(states.to(device)) # (B, action_size)\n",
    "            q_values[~before_legal_masks] = -torch.inf\n",
    "            q_value = q_values.gather(dim=1, index=actions) # tính q value với hành động đã chọn\n",
    "\n",
    "            print (f'type states {type(states)}, shape {states.shape}')\n",
    "            print (f'type actions {type(actions)}, shape {actions.shape}')\n",
    "            print (f'actions {actions}')\n",
    "            print (f'type reward {type(reward)}, shape {reward.shape}')\n",
    "            print (f'reward {reward}')\n",
    "            print (f'type next_states  {type(next_states)}, shape {next_states.shape}')\n",
    "            print (f'type before_legal_masks {type(before_legal_masks)}, shape {before_legal_masks.shape}')\n",
    "            print (f'before_legal_masks {before_legal_masks}')\n",
    "            print (f'type next_legal_masks {type(after_legal_mask)}, shape {after_legal_mask.shape}')\n",
    "            print (f'next_legal_masks {after_legal_mask}')\n",
    "            print (f'type dones {type(dones)}, shape {dones.shape}')\n",
    "            print (f'dones {dones}')\n",
    "            print (f'type q_target {type(q_target)}, shape {q_target.shape}')\n",
    "            print (f'q_target {q_target}')\n",
    "            print (f'type q_values {type(q_values)}, shape {q_values.shape}')\n",
    "            print (f'q_values {q_values[0, 75:85]}')\n",
    "            print (f'type q_value {type(q_value)}, shape {q_value.shape}')\n",
    "            print (f'q_values {q_value}')\n",
    "\n",
    "            loss = criterion(q_value, q_target)\n",
    "            print(f'loss {loss.item()}')\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # step += 1\n",
    "\n",
    "            if step % target_update_freq == 0:\n",
    "                step = 0\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "                # torch.save(target_net.state_dict(), 'DQN.pth')\n",
    "\n",
    "        done = True\n",
    "        epsilon = max(epsilon_final, epsilon*epsilon_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
